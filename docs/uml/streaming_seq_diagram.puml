@startuml
title Streaming message flow (POST /assistant) - current implementation

actor User
participant "Assistant UI\n(@assistant-ui/react)" as UI
participant "useAssistantTransportRuntime\n(MyRuntimeProvider.tsx)" as RT
participant "converter\n(MyMessageConverter.tsx)" as CONV
participant "FastAPI server.py\nPOST /assistant" as API
participant "assistant_stream_ce\ncreate_run / RunController" as STREAM
participant "LangGraph graph\n(make_agent_with_weather_tool)" as GRAPH
participant "MemorySaver\n(checkpointer)" as CP

User -> UI : Send message
UI -> RT : enqueue pending command\n(type = add-message)
RT -> API : POST /assistant\n(payload includes commands + state + metadata)

API -> STREAM : create_run(run_callback)

note right of API
run_callback (server.py) does:
- ensure controller.state exists
- for each command where type is add-message:
  - concatenate text parts
  - create HumanMessage(content=text, id=msg_id)
  - append model_dump to controller.state["messages"]
- build input_msg with key "messages"
- async iterate graph.astream with stream_mode "messages"
- append_langgraph_event writes events into controller.state
end note

STREAM -> GRAPH : astream(input_msg,\nconfig(thread_id),\nstream_mode=messages,\nsubgraphs=true)
GRAPH -> CP : read/write state\nscoped by thread_id
GRAPH --> STREAM : stream message events/chunks

STREAM --> API : streamed events
API --> RT : DataStreamResponse (stream)

RT -> CONV : converter(state, connectionMetadata)
CONV -> UI : toThreadMessages(allMessages)\n(convertLangChainMessages)
UI --> User : Incremental render updates

note over CONV
MyMessageConverter.tsx:
- serverMessages = state.messages or []
- pendingHumanMessages from pendingCommands
- merge pending human + server messages while sending
- convert with unstable_createMessageConverter(convertLangChainMessages)
end note

@enduml
